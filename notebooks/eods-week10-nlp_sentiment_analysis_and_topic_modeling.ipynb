{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Elements Of Data Science - F2020</center>\n",
    "# <center>Week 10: NLP, Sentiment Analysis and Topic Modeling<center>\n",
    "### <center>11/23/2020</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TODOs\n",
    "\n",
    "- Readings:\n",
    "  - [PDSH 5.11 k-Means](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)\n",
    "  - [Recommended] [PML Chapter 11: Working with Unlabeled Data - Clustering Analysis](https://ezproxy.cul.columbia.edu/login?qurl=https%3a%2f%2fsearch.ebscohost.com%2flogin.aspx%3fdirect%3dtrue%26db%3de025xna%26AN%3d1606531%26site%3dehost-live%26scope%3dsite%26ebv%3DEB%26ppid%3Dpp_347) except for last section on DBScan\n",
    "  - [Optional] [Data Science From Scratch Chap 22: Recommender Systems](https://ezproxy.cul.columbia.edu/login?qurl=https%3a%2f%2fsearch.ebscohost.com%2flogin.aspx%3fdirect%3dtrue%26db%3dnlebk%26AN%3d979529%26site%3dehost-live%26scope%3dsite%26ebv%3DEB%26ppid%3Dpp_267)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- HW3, Due Friday Dec 4th 11:59pm\n",
    "<br>\n",
    "\n",
    "- Answer and submit Quiz 10, **Sunday Nov 29th, 11:59pm ET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today\n",
    "\n",
    "- **Pipelines**\n",
    "- **NLP**\n",
    "- **Sentiment Analysis**\n",
    "- **Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# <center>Questions?</center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn\n",
    "\n",
    "- Pipelines are wrappers used to string together transformers and estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example from PML - scaling > feature extraction > classification\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "bc = load_breast_cancer()\n",
    "X,y = bc['data'],bc['target']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# make_pipeline: arguments in order of how they should be applied\n",
    "pipe_bc = make_pipeline(StandardScaler(),                    # center and scale data\n",
    "                        PCA(n_components=2),                 # extract 2 dimensions\n",
    "                        LogisticRegression(random_state=123) # classify using logistic regression\n",
    "                       )\n",
    "pipe_bc.fit(X_train,y_train) \n",
    "\n",
    "score = pipe_bc.score(X_test,y_test)\n",
    "print(f'test set accuracy: {score:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\"><img src=\"images/pipelines.png\" width=\"800px\"></div>\n",
    "\n",
    "<font size=6>From PML</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn: Named Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipeline: list of (name,object) pairs\n",
    "pipe_bc = Pipeline([('scale',StandardScaler()),\n",
    "                    ('pca',PCA(n_components=2)),\n",
    "                    ('lr',LogisticRegression(random_state=123)),\n",
    "                   ])\n",
    "\n",
    "pipe_bc.fit(X_train,y_train)\n",
    "\n",
    "score = pipe_bc.score(X_test,y_test)\n",
    "print(f'test set accuracy: {score:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0068728 ,  1.12126495]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access pipeline components by name like a dictionary\n",
    "pipe_bc['lr'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21777854, 0.08876361, 0.22663097, 0.22043131, 0.14913361,\n",
       "       0.23954684, 0.25974993, 0.26277752, 0.14518851, 0.06537618,\n",
       "       0.20775303, 0.0074925 , 0.21143104, 0.2018041 , 0.0165253 ,\n",
       "       0.17152404, 0.14891828, 0.18380569, 0.03639995, 0.09860293,\n",
       "       0.22726391, 0.09186544, 0.23623194, 0.22416772, 0.13445762,\n",
       "       0.21075345, 0.22996838, 0.25138607, 0.12409848, 0.13331693])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_bc['pca'].components_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pipelines in sklearn: GridSearch with Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 10, 'lr__penalty': 'l2', 'pca__n_components': 10}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# separate step-names and argument-names with double-underscore '__'\n",
    "params = {'pca__n_components':[2,10,100],\n",
    "          'lr__penalty':['none','l1','l2'],\n",
    "          'lr__C':[.01,1,10,100]}\n",
    "\n",
    "gscv = GridSearchCV(pipe_bc, params, cv=3, n_jobs=-1).fit(X_train,y_train)\n",
    "\n",
    "gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "score = gscv.score(X_test,y_test)\n",
    "print(f'test set accuracy: {score:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing (NLP)\n",
    "<br>\n",
    "\n",
    "- Analyzing and interacting with natural language\n",
    "- Python Libraries\n",
    "  - **sklearn**\n",
    "  - nltk\n",
    "  - **spaCy**\n",
    "  - gensim\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing (NLP)\n",
    "<br>\n",
    "\n",
    "- Many NLP Tasks\n",
    "\n",
    "  - **sentiment analysis**\n",
    "  - **topic modeling**\n",
    "  - entity detection\n",
    "  - machine translation\n",
    "  - natural language generation\n",
    "  - question answering\n",
    "  - relationship extraction\n",
    "  - automatic summarization\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: Python Builtin String Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D.S. is fun!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"D.S. is fun!\"\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('d.s. is fun!', 'D.S. IS FUN!')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.lower(),doc.upper()       # change capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['D.S.', 'is', 'fun!'], ['D', 'S', ' is fun!'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.split() , doc.split('.')  # split a string into parts (default is whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab|c|d'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(['ab','c','d'])      # join items in a list together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D|.|S|.| '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(doc[:5])             # a string itselft is treated like a list of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'  test   '.strip()           # remove whitespace from the beginning and end of a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- and many more, see [https://docs.python.org/3.8/library/string.html](https://docs.python.org/3.8/library/string.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: The Corpus\n",
    "<br>\n",
    "\n",
    "- **corpus:** collection of documents\n",
    "  - books\n",
    "  - articles\n",
    "  - reviews\n",
    "  - tweets\n",
    "  - resumes\n",
    "  - sentences?\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Doc Representation\n",
    "<br>\n",
    "\n",
    "- Documents usually represented as strings\n",
    "  - string: a sequence (list) of unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.S. is fun!\n",
      "It's  true.\n"
     ]
    }
   ],
   "source": [
    "doc = \"D.S. is fun!\\nIt's  true.\"\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"D|.|S|.| |i|s| |f|u|n|!|\\n|I|t|'|s| | |t|r|u|e|.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Need to split this up into parts (**tokens**)\n",
    "- Good job for **Regular Expressions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regular Expressions\n",
    "<br>\n",
    "\n",
    "- Strings that define search patterns over text\n",
    "- Useful for finding/replacing/grouping\n",
    "- python `re` library (others available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.S. is fun!\n",
      "It's  true.\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', ' ', '\\n', '  ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Find all of the whitespaces in doc\n",
    "# '\\s+' means \"one or more whitespace characters\"\n",
    "re.findall(r'\\s+',doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regular Expressions\n",
    "\n",
    "Just some of the special character definitions:\n",
    "    \n",
    "- `.` : any single character except newline (r'.' matches 'x')\n",
    "- `*` : match 0 or more repetitions (r'x*' matches 'x','xx','')\n",
    "- `+` : match 1 or more repetitions (r'x+' matches 'x','xx')\n",
    "- `?` : match 0 or 1 repetitions (r'x?' matches 'x' or '')\n",
    "<br>\n",
    "    \n",
    "- `^` : beginning of string (r'^D' matches 'D.S.')\n",
    "- `$` : end of string (r'fun!$' matches 'DS is fun!'`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regular Expression Cont.\n",
    "<br>\n",
    "\n",
    "- `[]` : a set of characters (^ as first element = not)\n",
    "- `\\s` : whitespace character (Ex: [ \\t\\n\\r\\f\\v])\n",
    "- `\\S` : non-whitespace character (Ex: [^ \\t\\n\\r\\f\\v])\n",
    "- `\\w` : word character (Ex: [a-zA-Z0-9_])\n",
    "- `\\W` : non-word character\n",
    "- `\\b` : boundary between \\w and \\W\n",
    "- and many more!\n",
    "<br>\n",
    "\n",
    "- See [regex101.com](https://regex101.com) for examples and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Tokenization\n",
    "\n",
    "- **tokens:** strings that make up a document ('the', 'cat',...)\n",
    "- **tokenization:** convert a document into tokens\n",
    "- **vocabulary:** set of unique tokens (terms) in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D.S.', 'is', 'fun!', \"It's\", 'true.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split on whitespace\n",
    "re.split(r'\\s+', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is', 'fun', 'It', 'true']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find tokens of length 2+ word characters\n",
    "re.split('\\b\\w\\w+\\b',doc)\n",
    "['is', 'fun', 'It', 'true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D.S', 'is', 'fun', \"It's\", 'true']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find tokens of length 2+ non-space characters\n",
    "\n",
    "re.findall(r\"\\b\\S\\S+\\b\", doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP:Tokenization\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\"><img src=\"images/spacy_tokenization.svg\" width=\"1000px\"></align>\n",
    "\n",
    "<font size=5>From [https://spacy.io/usage/linguistic-features](https://spacy.io/usage/linguistic-features)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Other Preprocessing\n",
    "<br>\n",
    "\n",
    "- lowercase\n",
    "- remove special characters\n",
    "- add <START>, <END> tags\n",
    "- stemming: cut off beginning or ending of word\n",
    "  - 'studies' becomes 'studi'\n",
    "  - 'studying' becomes 'study'\n",
    "- lemmatization: perform morphological analysis\n",
    "  - 'studies' becomes 'study'\n",
    "  - 'studying' becomes 'study'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Bag of Words\n",
    "    \n",
    "- BOW representation: ignore token order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d.s', 'fun', 'is', \"it's\", 'true']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(re.findall(r'\\b\\S\\S+\\b', doc.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: n-Grams\n",
    "\n",
    "- Unigram: single token\n",
    "- Bigram: combination of two ordered tokens\n",
    "- n-Gram: combination of n ordered tokens\n",
    "- The larger n is, the larger the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>_data', 'data_science', 'science_is', 'is_fun', 'fun_<end>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bigram example:\n",
    "tokens = '<start> data science is fun <end>'.split()\n",
    "[tokens[i]+'_'+tokens[i+1] for i in range(len(tokens)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: TF and DF\n",
    "\n",
    "- **Term Frequency:** number of times term is seen per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blue', 'green', 'red']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['red green blue', 'red blue blue']\n",
    "\n",
    "#Vocabulary\n",
    "vocab = sorted(set(' '.join(corpus).split()))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blue</th>\n",
       "      <th>green</th>\n",
       "      <th>red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      blue  green  red\n",
       "doc1   1.0    1.0  1.0\n",
       "doc2   2.0    0.0  1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TF\n",
    "from collections import Counter\n",
    "tf = np.zeros((len(corpus),len(vocab)))\n",
    "for i,doc in enumerate(corpus):\n",
    "    for j,term in enumerate(vocab):\n",
    "        tf[i,j] = Counter(doc.split())[term]\n",
    "tf = pd.DataFrame(tf,index=['doc1','doc2'],columns=vocab)\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: TF and DF\n",
    "\n",
    "- **Document Frequency:** number of documents containing each term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blue     2\n",
       "green    1\n",
       "red      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DF\n",
    "tf.astype(bool).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Stopwords\n",
    "\n",
    "- terms that have high (or very low) DF and aren't informative\n",
    "  - common engish terms (ex: 'a', 'the','in',...)\n",
    "  - domain specific (ex, in class slides: 'data_science')\n",
    "  - often removed prior to analysis\n",
    "  - in sklearn\n",
    "    - `min_df`, an integer > 0, keep terms that occur in at at least n documents\n",
    "    - `max_df`, a float in (0,1], keep terms that occur in less than f% of total documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: CountVectorizer in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['blue green red', 'blue green green']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvect = CountVectorizer(lowercase=True,    # default, transform all docs to lowercase\n",
    "                        ngram_range=(1,1), # default, only unigrams\n",
    "                        min_df=1,          # default, keep all terms\n",
    "                        max_df=1.0,        # default, keep all terms\n",
    "                       )\n",
    "X_cv = cvect.fit_transform(corpus)\n",
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue', 0), ('green', 1), ('red', 2)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learned vocabulary, sorted by column mapping id\n",
    "sorted(cvect.vocabulary_.items(),key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1],\n",
       "        [1, 2, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term frequencies\n",
    "X_cv.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['blue', 'green', 'red'], dtype='<U5'),\n",
       " array(['blue', 'green'], dtype='<U5')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping back to terms via vocabulary mapping\n",
    "cvect.inverse_transform(X_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: TfIdf\n",
    "\n",
    "- What if some terms are still uninformative?\n",
    "- Can we downweight terms that occur in many documents?\n",
    "- **Term Frequency * Inverse Document Frequency (tf-idf)**\n",
    "  - $\\text{tf-idf}(t,d) = \\text{tf}(t, d) \\times \\text{idf}(t)$\n",
    "  - $\\text{idf}(t) = \\log \\frac{1+n}{1+\\text{df}(t)} + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue', 0), ('green', 1), ('red', 2)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvect = TfidfVectorizer(norm='l2') # by default, also doing l2 normalization\n",
    "\n",
    "X_tfidf = tfidfvect.fit_transform(corpus)\n",
    "sorted(tfidfvect.vocabulary_.items(),key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.50154891, 0.50154891, 0.70490949],\n",
       "        [0.4472136 , 0.89442719, 0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 1., 1.],\n",
       "        [1., 2., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can also use to get term frequencies by setting use_idf to False and norm to none\n",
    "TfidfVectorizer(use_idf=False, norm=None).fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP: Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rec.sport.baseball\n",
      "--------------------------------------------------\n",
      "From: dougb@comm.mot.com (Doug Bank)\n",
      "Subject: Re: Info needed for Cleveland tickets\n",
      "Reply-To: dougb@ecs.comm.mot.com\n",
      "Organization: Motorola Land Mobile Products Sector\n",
      "Distribution: usa\n",
      "Nntp-Posting-Host: 145.1.146.35\n",
      "Lines: 17\n",
      "\n",
      "In article <1993Apr1.234031.4950@leland.Stanford.EDU>, bohnert@leland.Stanford.EDU (matthew bohnert) writes:\n",
      "\n",
      "|> I'm going to be in Cleveland Thursday, April 15 to Sunday, April 18.\n",
      "|> Does anybody know if the Tribe will be in town on those dates, and\n",
      "|> if so, who're they playing and if tickets are available?\n",
      "\n",
      "The tribe will be in town from April 16 to the 19th.\n",
      "There\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "ngs = fetch_20newsgroups(categories=['rec.sport.baseball','rec.sport.hockey']) # dataset has 20 categories, only get two\n",
    "\n",
    "docs_ngs = ngs['data']                         # get documents (emails)\n",
    "y_ngs = ngs['target']                          # get targets ([0,1])\n",
    "target_names_ngs = ngs['target_names']         # get target names (['rec.autos','sci.space'])\n",
    "\n",
    "print(y_ngs[0], target_names_ngs[y_ngs[0]])  # print target int and target name\n",
    "print('-'*50)                                  # print a string of 50 dashes\n",
    "print(docs_ngs[0].strip()[:600])               # print beginning characters of first doc, after stripping whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Transform Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(897, 3676)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "docs_ngs_train,docs_ngs_test,y_ngs_train,y_ngs_test = train_test_split(docs_ngs,y_ngs)\n",
    "\n",
    "vect = TfidfVectorizer(lowercase=True,\n",
    "                       min_df=5,       # occur in at least 5 documents\n",
    "                       max_df=0.8,     # occur in at most 80% of documents\n",
    "                       token_pattern='\\\\b\\\\S\\\\S+\\\\b',  # tokens of at least 2 non-space characters\n",
    "                       ngram_range=(1,1),  # only unigrams\n",
    "                       use_idf=False,  # term frequency counts instead of tf-idf\n",
    "                       norm=None       # do not normalize\n",
    "                      )\n",
    "X_ngs_train = vect.fit_transform(docs_ngs_train)\n",
    "X_ngs_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('university', 3428),\n",
       " ('computing', 870),\n",
       " ('center', 744),\n",
       " ('just', 1841),\n",
       " ('distribution', 1082)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first few terms in learned vocabulary\n",
    "list(vect.vocabulary_.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upcomming', 'pony.1993apr15.223040.8733', 'gidp', 'alignment', 'memorable']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first few terms in learned stopword list\n",
    "list(vect.stop_words_)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['university', 'computing', 'center', 'just', 'distribution', 'usa'],\n",
       "      dtype='<U77')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first few terms in BOW representation of first document\n",
    "vect.inverse_transform(X_ngs_train[0])[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Train and Evaluate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy cv accuracy: 0.51 +- 0.00\n",
      "lr    cv accuracy: 0.95 +- 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "scores_dummy = cross_val_score(DummyClassifier(strategy='most_frequent'),X_ngs_train,y_ngs_train)\n",
    "scores_lr = cross_val_score(LogisticRegression(),X_ngs_train,y_ngs_train)\n",
    "\n",
    "print(f'dummy cv accuracy: {scores_dummy.mean():0.2f} +- {scores_dummy.std():0.2f}')\n",
    "print(f'lr    cv accuracy: {scores_lr.mean():0.2f} +- {scores_lr.std():0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline accuracy on training set: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# use Pipeline instead of make_pipeline to add names to the steps\n",
    "# (name,object) tuple pairs for each step\n",
    "ngs_pipe = Pipeline([('vect',TfidfVectorizer(lowercase=True,\n",
    "                                             min_df=5,\n",
    "                                             max_df=0.8,\n",
    "                                             token_pattern='\\\\b\\\\S\\\\S+\\\\b',\n",
    "                                             ngram_range=(1,1),\n",
    "                                             use_idf=False,\n",
    "                                             norm=None )\n",
    "                     ),   \n",
    "                     ('lr',LogisticRegression())\n",
    "                    ])\n",
    "\n",
    "ngs_pipe.fit(docs_ngs_train,y_ngs_train) # pass in docs, not transformed X\n",
    "\n",
    "score_ngs = ngs_pipe.score(docs_ngs_train,y_ngs_train)\n",
    "print(f'pipeline accuracy on training set: {score_ngs:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipe cv accuracy: 0.94 +- 0.01\n"
     ]
    }
   ],
   "source": [
    "scores_pipe = cross_val_score(ngs_pipe,docs_ngs_train,y_ngs_train) \n",
    "print(f'pipe cv accuracy: {scores_pipe.mean():0.2f} +- {scores_pipe.std():0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('university', 3428), ('computing', 870), ('center', 744)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngs_pipe['vect'].vocabulary_.items())[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Add Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline accuracy on training set: 0.97\n",
      "pipe cv accuracy: 0.92 +- 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "ngs_pipe = Pipeline([('vect',TfidfVectorizer(lowercase=True,\n",
    "                                             min_df=5,\n",
    "                                             max_df=0.8,\n",
    "                                             token_pattern='\\\\b\\\\S\\\\S+\\\\b',\n",
    "                                             ngram_range=(1,1),\n",
    "                                             use_idf=False,\n",
    "                                             norm=None )\n",
    "                     ),   \n",
    "                     ('fs',SelectFromModel(estimator=LogisticRegression(C=.1,penalty='l1',solver='liblinear'))),\n",
    "                     ('lr',LogisticRegression())\n",
    "                    ])\n",
    "ngs_pipe.fit(docs_ngs_train,y_ngs_train)\n",
    "print(f'pipeline accuracy on training set: {ngs_pipe.score(docs_ngs_train,y_ngs_train):0.2f}')\n",
    "scores_pipe = cross_val_score(ngs_pipe,docs_ngs_train,y_ngs_train) \n",
    "print(f'pipe cv accuracy: {scores_pipe.mean():0.2f} +- {scores_pipe.std():0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Example: Grid Search with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fs__estimator__C': 0.1, 'lr__C': 0.1, 'vect__norm': None, 'vect__use_idf': True}\n",
      "gscsv best cv accuracy : 0.95\n",
      "gscsv test set accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "params = {'vect__use_idf':[True,False],\n",
    "          'vect__norm':['l1','l2',None],\n",
    "          'fs__estimator__C':[.01,.1,1,10],\n",
    "          'lr__C':[.01,.1,1,10]}\n",
    "\n",
    "gscv = GridSearchCV(ngs_pipe, params, cv=3, n_jobs=-1).fit(docs_ngs_train,y_ngs_train)\n",
    "\n",
    "print(gscv.best_params_)\n",
    "print(f'gscsv best cv accuracy : {gscv.best_score_:0.2f}')\n",
    "print(f'gscsv test set accuracy: {gscv.score(docs_ngs_test,y_ngs_test):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentiment Analysis and sklearn\n",
    "<br>\n",
    "\n",
    "- determine sentiment/opinion from unstructured test\n",
    "- usually positive/negative, but is domain specific\n",
    "- can be treated as a classification task (with a target, using all of the tools we know)\n",
    "- can also be treated as a linguistic task (sentence parsing)\n",
    "<br>\n",
    "\n",
    "- Example: determine sentiment of movie reviews\n",
    "- see sentiment_analysis_example.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling\n",
    "\n",
    "- What topics are our documents composed of?\n",
    "- How much of each topic does each document contain?\n",
    "- Can we represent documents using topic weights? (dimensionality reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is topic modeling?\n",
    "- How does Latent Dirichlet Allocation (LDA) work?\n",
    "- How to train and use LDA with sklearn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Topic Modeling?\n",
    "<br>\n",
    "\n",
    "- **topic:** a collection of related words\n",
    "- A document can be composed of several topics\n",
    "<br>\n",
    "\n",
    "- Given a collection of documents, we can ask:\n",
    "  - **What terms make up each topic?** (per topic term distribution)\n",
    "  - **What topics make up each document?** (per document topic distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "- Unsupervised method for determining topics and topic assignments\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align=\"center\"><img src=\"images/lda_blei.jpg\" width=\"1100px\"></div>\n",
    "\n",
    "<font size=5>From David Blei</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Example\n",
    "\n",
    "- Guessing some **topics** (per topic term distribution $\\phi$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6)\n"
     ]
    }
   ],
   "source": [
    "vocab = ['baseball','cat','dog','pet','played','tennis']\n",
    "\n",
    "V = len(vocab) # size of vocabulary\n",
    "\n",
    "K = 2 # number of topics\n",
    "\n",
    "# the probability of each term given topic 1 (high for sports terms)\n",
    "topic_1 = [.33,   0,   0,   0, .33, .33]\n",
    "\n",
    "# the probability of each term given topic 2 (high for pet terms)\n",
    "topic_2 = [  0, .25, .25, .25, .25,   0]\n",
    "\n",
    "# per topic term distributions\n",
    "phi = [topic_1, topic_2]\n",
    "\n",
    "print(np.array(phi).shape) # K x V (number of topics x size of vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling: Example\n",
    "\n",
    "- Guessing the per document topic distributions $\\theta$ given the **topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# recall\n",
    "vocab = ['baseball','cat','dog','pet','played','tennis']\n",
    "\n",
    "phi = [[.33,   0,   0,   0, .33, .33],\n",
    "       [  0, .25, .25, .25, .25,   0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "corpus = ['the dog and cat played tennis',\n",
    "          'tennis and baseball are sports',\n",
    "          'a dog or a cat can be a pet']\n",
    "\n",
    "# per document topic distributions\n",
    "theta = [[.50, .50],\n",
    "         [.99, .01],\n",
    "         [.01, .99]]\n",
    "\n",
    "print(np.array(theta).shape) # M x K (number of documents x number of topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling With LDA\n",
    "\n",
    "- Given\n",
    "  - a set of documents\n",
    "  - a number of topics $k$\n",
    "<br>\n",
    "\n",
    "- Learn\n",
    "  - the **per topic term distributions $\\varphi$ (phi)**, size: $k \\times V$\n",
    "  - the **per document topic distributions $\\theta$ (theta)**, size: $n \\times k$\n",
    "<br>\n",
    "\n",
    "- How to learn $\\phi$ and $\\theta$:\n",
    "  - Latent Dirichlet Allocation (LDA)\n",
    "  - generative statistical model\n",
    "  - Blei, D., Ng, A., Jordan, M. Latent Dirichlet allocation. J. Mach. Learn. Res. 3 (Jan 2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topic Modeling With LDA\n",
    "\n",
    "- Uses for $\\varphi$ (phi), the per topic word distributions:\n",
    "  - infering labels for topics\n",
    "  - word clouds\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Uses for $\\theta$ (theta), the per document topic distributions:\n",
    "  - dimentionality reduction\n",
    "  - clustering\n",
    "  - similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from all 20 newsgroups\n",
    "newsgroups = fetch_20newsgroups()\n",
    "ngs_all = newsgroups.data\n",
    "len(ngs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 4256)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform documents using tf-idf\n",
    "tfidf = TfidfVectorizer(token_pattern=r'\\b[a-zA-Z0-9-][a-zA-Z0-9-]+\\b',min_df=50, max_df=.2)\n",
    "X_tfidf = tfidf.fit_transform(ngs_all)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '01', '02', '03', '04', '05', '06', '07', '08']\n",
      "['yours', 'yourself', 'ysu', 'zealand', 'zero', 'zeus', 'zip', 'zone', 'zoo', 'zuma']\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "print(feature_names[:10])\n",
    "print(feature_names[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA with sklearn Cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# create model with 20 topics\n",
    "lda = LatentDirichletAllocation(n_components=20,  # the number of topics\n",
    "                                n_jobs=-1,        # use all cpus\n",
    "                                random_state=123) # for reproducability\n",
    "\n",
    "# learn phi (lda.components_) and theta (X_lda)\n",
    "# this will take a while!\n",
    "X_lda = lda.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: tchen@magnus.acs.ohio-state.edu (Tsung-Kun Chen)\\nSubject: ** Software forsale (lots) **\\nNntp-P'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngs_all[100][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01, 0.01, 0.01, 0.1 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "       0.01, 0.01, 0.01, 0.38, 0.01, 0.14, 0.01, 0.01, 0.28])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(X_lda[100],2) # lda representation of document_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 19, 16])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: since this is unsupervised, these numbers may change\n",
    "np.argsort(X_lda[100])[::-1][:3] # the top topics of document_100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA: Per Topic Term Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# a utility function to print out the most likely terms for each topic\n",
    "# taken from https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic {:#2d}: \".format(topic_idx)\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0: uga ai georgia covington mcovingt\n",
      "Topic  1: digex access turkish armenian armenians\n",
      "Topic  2: god jesus bible christians christian\n",
      "Topic  3: values objective frank morality ap\n",
      "Topic  4: ohio-state magnus acs ohio cis\n",
      "Topic  5: caltech keith sandvik livesey sgi\n",
      "Topic  6: stratus msg usc indiana sw\n",
      "Topic  7: alaska uci aurora colostate nsmca\n",
      "Topic  8: wpi radar psu psuvm detector\n",
      "Topic  9: columbia utexas gatech cc prism\n",
      "Topic 10: scsi upenn simms ide bus\n",
      "Topic 11: nhl team mit players hockey\n",
      "Topic 12: lehigh duke jewish adobe ns1\n",
      "Topic 13: henry toronto zoo ti dseg\n",
      "Topic 14: sale card thanks please mac\n",
      "Topic 15: virginia joel hall doug douglas\n",
      "Topic 16: ca his new cs should\n",
      "Topic 17: cleveland cwru freenet cramer ins\n",
      "Topic 18: pitt gordon geb banks cs\n",
      "Topic 19: windows file window files thanks\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda,feature_names,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA Review\n",
    "\n",
    "- What did we learn?\n",
    "  - per document topic distributions\n",
    "  - per topic term distributions\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- What can we use this for?\n",
    "  - Dimensionality Reduction/Feature Extraction!\n",
    "  - investigate topics (much like PCA components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other NLP Features\n",
    "\n",
    "- Part of Speech tags\n",
    "- Dependency Parsing\n",
    "- Entity Detection\n",
    "- Word Vectors\n",
    "- See spaCy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"N.Y.C.|is|n't|in|New|Jersey|.\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#first run\n",
    "#%run -m spacy download en_core_web_sm\n",
    "try:\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "except OSError as e:\n",
    "    print('Need to run the following line in a new cell:')\n",
    "    print('%run -m spacy download en_core_web_sm')\n",
    "    print('or the following line from the commandline with eods-f20 activated:')\n",
    "    print('python -m spacy download en_core_web_sm')\n",
    "    \n",
    "parsed = nlp(\"N.Y.C. isn't in New Jersey.\")\n",
    "'|'.join([token.text for token in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text    lemma   pos   is_stop\n",
      "------------------------------\n",
      "Apple   Apple   PROPN False\n",
      "is      be      AUX   True\n",
      "looking look    VERB  False\n",
      "at      at      ADP   True\n",
      "buying  buy     VERB  False\n",
      "U.K.    U.K.    PROPN False\n",
      "startup startup NOUN  False\n",
      "for     for     ADP   True\n",
      "$       $       SYM   False\n",
      "1       1       NUM   False\n",
      "billion billion NUM   False\n",
      ".       .       PUNCT False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
    "\n",
    "print(f\"{'text':7s} {'lemma':7s} {'pos':5s} {'is_stop'}\")\n",
    "print('-'*30)\n",
    "for token in doc:\n",
    "    print(f'{token.text:7s} {token.lemma_:7s} {token.pos_:5s} {token.is_stop}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"172fceae52634ad99ce86ba00b207be3-0\" class=\"displacy\" width=\"1975\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">billion.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-172fceae52634ad99ce86ba00b207be3-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-172fceae52634ad99ce86ba00b207be3-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-172fceae52634ad99ce86ba00b207be3-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-172fceae52634ad99ce86ba00b207be3-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-172fceae52634ad99ce86ba00b207be3-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-172fceae52634ad99ce86ba00b207be3-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-172fceae52634ad99ce86ba00b207be3-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-172fceae52634ad99ce86ba00b207be3-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-172fceae52634ad99ce86ba00b207be3-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-172fceae52634ad99ce86ba00b207be3-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-172fceae52634ad99ce86ba00b207be3-0-5\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-172fceae52634ad99ce86ba00b207be3-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,266.5 L1103.0,254.5 1087.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-172fceae52634ad99ce86ba00b207be3-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-172fceae52634ad99ce86ba00b207be3-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-172fceae52634ad99ce86ba00b207be3-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-172fceae52634ad99ce86ba00b207be3-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-172fceae52634ad99ce86ba00b207be3-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-172fceae52634ad99ce86ba00b207be3-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-172fceae52634ad99ce86ba00b207be3-0-9\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,2.0 1800.0,2.0 1800.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-172fceae52634ad99ce86ba00b207be3-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1800.0,266.5 L1808.0,254.5 1792.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ent.text,ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Word Vectors\n",
    "\n",
    "- word2vec\n",
    "- shallow neural net\n",
    "- predict a word given the surrounding context (SkipGram or CBOW)\n",
    "- words used in similar context should have similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# Need either the _md or _lg models to get vector information\n",
    "# Note: this takes a while!\n",
    "%run -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Baseball', (300,), [0.55838, 0.42791, -0.11687])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md') # _lg has a larger vocabulary\n",
    "\n",
    "doc = nlp('Baseball is played on a diamond.')\n",
    "doc[0].text, doc[0].vector.shape, list(doc[0].vector[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy: Multiple Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use nlp.pipe to transform multiple docs at once\n",
    "docs = list(nlp.pipe(['Baseball is played on a diamond.',\n",
    "                      'Hockey is played on ice.',\n",
    "                      'Diamonds are clear as ice.']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['1.00', '0.85', '0.76'],\n",
       "       ['0.85', '1.00', '0.77'],\n",
       "       ['0.76', '0.77', '1.00']], dtype='<U4')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using average of token vectors for each document.\n",
    "np.array([['{:.2f}'.format(docs[i].similarity(docs[j])) for j in range(3)]\n",
    "          for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning Sequences\n",
    "\n",
    "- Hidden Markov Models\n",
    "- Conditional Random Fields\n",
    "- Recurrant Neural Networks\n",
    "- LSTM\n",
    "- [BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Review\n",
    "\n",
    "- corpus,tokens, vocabulary, terms, n-grams, stopwords\n",
    "- tokenization\n",
    "- term frequency (TF), document frequency (DF)\n",
    "- TF vs TF-IDF\n",
    "- sentiment analysis\n",
    "- topic modeling\n",
    "<br>\n",
    "\n",
    "- POS\n",
    "- Dependency Parsing\n",
    "- Entity Extraction\n",
    "- Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# <center>Questions?</center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LDA Plate Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"images/Smoothed_LDA.png\" width=\"400px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<font size=5>\n",
    "    \n",
    "**K** :  number of topics\n",
    "\n",
    "**$\\varphi$** : per topic term distributions\n",
    "\n",
    "**$\\beta$**  : parameters for word distribution die factory, length = V (size of vocab)\n",
    "\n",
    "**M**     : number of documents\n",
    "\n",
    "**N**     : number of words/tokens in each document\n",
    "\n",
    "**$\\theta$** : per document topic distributions\n",
    "\n",
    "**$\\alpha$** : parameters for topic die factory, length = K (number of topics)\n",
    "\n",
    "**z** : topic indexes\n",
    "\n",
    "**w** : observed tokens\n",
    "\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "eods-f20",
   "language": "python",
   "name": "eods-f20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
